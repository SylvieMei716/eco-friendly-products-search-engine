{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "import os\n",
    "from document_preprocessor import RegexTokenizer\n",
    "from indexing import Indexer, IndexType, BasicInvertedIndex\n",
    "from ranker import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/'  # TODO: Set this to the path to your data folder\n",
    "CACHE_PATH = '__pycache__/'  # Set this to the path of the cache folder\n",
    "\n",
    "BEAUTY_PATH = DATA_PATH + 'meta_All_Beauty.jsonl.gz'\n",
    "FASHION_PATH = DATA_PATH + 'meta_Amazon_Fashion.jsonl.gz'\n",
    "COMBINE_PATH = DATA_PATH + 'Beauty_and_Fashion.jsonl.gz'\n",
    "STOPWORD_PATH = DATA_PATH + 'stopwords.txt'\n",
    "MAIN_INDEX = 'main_index'\n",
    "TITLE_INDEX = 'title_index'\n",
    "N_DOC_NEEDED = 50\n",
    "DOCID_TO_TITLE_PATH = CACHE_PATH + 'docid_to_title.pkl'\n",
    "DOCID_TO_LINK_PATH = CACHE_PATH + 'docid_to_link.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 543 stopwords.\n"
     ]
    }
   ],
   "source": [
    "# Load stopwords\n",
    "stopwords = set()\n",
    "with open(STOPWORD_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        stopwords.add(line.strip())\n",
    "        \n",
    "print('Loaded', len(stopwords), 'stopwords.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 493293 items in total to data/Beauty_and_Fashion.jsonl.gz from both Beauty and Fashion.\n"
     ]
    }
   ],
   "source": [
    "# Load two categories' items into one dataset\n",
    "item_cnt = 0\n",
    "keys_to_keep = [\"main_category\", \"title\", \"average_rating\", \"rating_number\", \"price\", \"images\", \"details\"]\n",
    "\n",
    "def process_dataset(input_path, output_file, item_cnt):\n",
    "    with gzip.open(input_path, 'rt') as infile:\n",
    "        for line in infile:\n",
    "            data = json.loads(line)\n",
    "            if data['description'] == [] and data['features'] == []:\n",
    "                continue\n",
    "            item_cnt += 1\n",
    "            filtered_data = {key:data[key] for key in keys_to_keep if key in data}\n",
    "            filtered_data['docid'] = item_cnt\n",
    "            filtered_data['description'] = \" \".join(data['features'] + data['description'])\n",
    "            filtered_data['link'] = \"https://www.amazon.com/dp/\" + data['parent_asin']\n",
    "            output_file.write(json.dumps(filtered_data) + '\\n')\n",
    "            \n",
    "    return item_cnt\n",
    "\n",
    "with gzip.open(COMBINE_PATH, 'wt') as outfile:\n",
    "    item_cnt = process_dataset(BEAUTY_PATH, outfile, item_cnt)\n",
    "    N_BEAUTY = item_cnt\n",
    "    item_cnt = process_dataset(FASHION_PATH, outfile, item_cnt)\n",
    "    N_FASHION = item_cnt - N_BEAUTY\n",
    "    \n",
    "print(f'Added {item_cnt} items in total to {COMBINE_PATH} from both Beauty and Fashion.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading indexes...\n",
      "6212.378998041153\n"
     ]
    }
   ],
   "source": [
    "print('Loading indexes...')\n",
    "preprocessor = RegexTokenizer('\\w+')\n",
    "import time\n",
    "start_time = time.time()\n",
    "if not os.path.exists(MAIN_INDEX):\n",
    "    main_index = Indexer.create_index(\n",
    "        IndexType.BasicInvertedIndex, COMBINE_PATH, preprocessor,\n",
    "        stopwords, 3, text_key='description', max_docs=493293\n",
    "    )\n",
    "    main_index.save(MAIN_INDEX)\n",
    "else:\n",
    "    main_index = BasicInvertedIndex()\n",
    "    main_index.load(MAIN_INDEX)\n",
    "print(time.time() - start_time)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19043.13917684555\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "if not os.path.exists(TITLE_INDEX):\n",
    "    title_index = Indexer.create_index(\n",
    "        IndexType.BasicInvertedIndex, COMBINE_PATH, preprocessor, \n",
    "        stopwords, 2, max_docs=493293,\n",
    "        text_key='title'\n",
    "    )\n",
    "    title_index.save(TITLE_INDEX)\n",
    "else:\n",
    "    title_index = BasicInvertedIndex()\n",
    "    title_index.load(TITLE_INDEX)\n",
    "print(time.time() - start_time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import tqdm\n",
    "if not os.path.exists(DOCID_TO_TITLE_PATH):\n",
    "    docid_to_title = {}\n",
    "    with gzip.open(COMBINE_PATH, mode = 'rt', newline = '') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            docid_to_title[data['docid']] = data['title']\n",
    "    pickle.dump(docid_to_title,\n",
    "                open(DOCID_TO_TITLE_PATH, 'wb')\n",
    "    )\n",
    "else:\n",
    "    docid_to_title = pickle.load(open(DOCID_TO_TITLE_PATH, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DOCID_TO_LINK_PATH):\n",
    "    docid_to_link = {}\n",
    "    with gzip.open(COMBINE_PATH, mode = 'rt', newline = '') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            docid_to_link[data['docid']] = data['link']\n",
    "    pickle.dump(docid_to_link,\n",
    "                open(DOCID_TO_LINK_PATH, 'wb')\n",
    "    )\n",
    "else:\n",
    "    docid_to_link = pickle.load(open(DOCID_TO_LINK_PATH, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = Ranker(main_index, preprocessor, stopwords, BM25(main_index))\n",
    "\n",
    "import pandas as pd\n",
    "beauty_queries = [\"Hydrating face serum\",\n",
    "\"Organic lip balm\",\n",
    "\"Sunscreen spf 50\",\n",
    "\"Matte foundation\",\n",
    "\"Hair repair oil\",\n",
    "\"Anti-aging night cream for sensitive skin\",\n",
    "\"Cruelty-free makeup set\",\n",
    "\"Gentle facial cleanser with natural ingredients\",\n",
    "\"Long-lasting waterproof mascara\",\n",
    "\"Shampoo and conditioner set for curly hair\",\n",
    "]\n",
    "fashion_queries = [\"Maxi dress\",\n",
    "\"Crop top\",\n",
    "\"V-neck t-shirt\",\n",
    "\"Gray baggy jeans\",\n",
    "\"Wool scarf\",\n",
    "\"Running shoes with cushions\",\n",
    "\"Lightweight travel backpack\",\n",
    "\"High-waisted leggings with pockets\",\n",
    "\"Casual blazer for men in slim fit style\",\n",
    "\"Kidsâ€™ winter coat waterproof\",\n",
    "]\n",
    "\n",
    "for beauty_query in beauty_queries:\n",
    "    doc_lst = ranker.query(beauty_query)[:N_DOC_NEEDED]\n",
    "    df = pd.DataFrame(columns=['query','title','docid','link','rel'])\n",
    "    for i in range(len(doc_lst)):\n",
    "        df.loc[i] = [beauty_query, docid_to_title[doc_lst[i][0]], doc_lst[i][0], docid_to_link[doc_lst[i][0]], None]\n",
    "    df.to_csv(beauty_query+'.csv', index=False)\n",
    "\n",
    "for fashion_query in fashion_queries:\n",
    "    doc_lst = ranker.query(fashion_query)[:N_DOC_NEEDED]\n",
    "    df = pd.DataFrame(columns=['query','title','docid','link','rel'])\n",
    "    for i in range(len(doc_lst)):\n",
    "        df.loc[i] = [fashion_query, docid_to_title[doc_lst[i][0]], doc_lst[i][0], docid_to_link[doc_lst[i][0]], None]\n",
    "    df.to_csv(fashion_query+'.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
